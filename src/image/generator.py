"""
–ú–æ–¥—É–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª–æ–∫–∞–ª—å–Ω—ã—Ö Stable Diffusion –º–æ–¥–µ–ª–µ–π.
–°–æ–∑–¥–∞–µ—Ç –ø–æ–∑–¥—Ä–∞–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ —Å AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π.
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ multi-GPU –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥–∞.
"""

import asyncio
import time
import gc
import re
import shutil
from pathlib import Path
from typing import Optional, List, Dict, Any
from PIL import Image, ImageDraw, ImageFont
from contextlib import asynccontextmanager

from src.utils.config import config
from src.utils.logger import get_image_logger
from transformers import MarianMTModel, MarianTokenizer

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
logger = get_image_logger()

class TranslatorPool:
    """–ü—É–ª –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞."""
    
    def __init__(self, gpu_devices: List[str]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É–ª–∞ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤.
        
        Args:
            gpu_devices: –°–ø–∏—Å–æ–∫ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤
        """
        self.gpu_devices = gpu_devices
        self.model_name = "Helsinki-NLP/opus-mt-ru-en"
        self.tokenizers: Dict[str, Any] = {}
        self.models: Dict[str, Any] = {}
        self.available_devices = asyncio.Queue(maxsize=len(gpu_devices))
        self._initialized = False
        
        logger.info(f"üî§ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø—É–ª –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤ —Å {len(gpu_devices)} —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏: {gpu_devices}")
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è GPU."""
        if self._initialized:
            return
        
        logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –≤—Å–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞...")
        
        for device in self.gpu_devices:
            try:
                tokenizer, model = await self._load_translator_for_device(device)
                if tokenizer and model:
                    self.tokenizers[device] = tokenizer
                    self.models[device] = model
                    await self.available_devices.put(device)
                    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è {device}")
                else:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è {device}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è {device}: {e}")
        
        self._initialized = True
        logger.info(f"üöÄ –ü—É–ª –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å {len(self.models)} –∞–∫—Ç–∏–≤–Ω—ã–º–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏")
    
    async def _load_translator_for_device(self, device: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞."""
        try:
            import warnings
            warnings.filterwarnings("ignore", message=".*add_prefix_space.*")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            loop = asyncio.get_event_loop()
            
            tokenizer = await loop.run_in_executor(
                None,
                MarianTokenizer.from_pretrained,
                self.model_name
            )
            
            model = await loop.run_in_executor(
                None,
                MarianMTModel.from_pretrained,
                self.model_name
            )
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if device != "cpu":
                model = model.to(device)
            
            return tokenizer, model
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞ –¥–ª—è {device}: {e}")
            return None, None
    
    @asynccontextmanager
    async def acquire_translator(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞ –∏–∑ –ø—É–ª–∞."""
        if not self._initialized:
            await self.initialize()
        
        # –ñ–¥–µ–º —Å–≤–æ–±–æ–¥–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        device = await self.available_devices.get()
        tokenizer = self.tokenizers.get(device)
        model = self.models.get(device)
        
        if not tokenizer or not model:
            await self.available_devices.put(device)
            raise RuntimeError(f"–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ –¥–ª—è {device} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        try:
            logger.debug(f"üîí –ü–æ–ª—É—á–µ–Ω –¥–æ—Å—Ç—É–ø –∫ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫—É {device}")
            yield device, tokenizer, model
        finally:
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –≤ –ø—É–ª
            self._cleanup_device_memory(device)
            await self.available_devices.put(device)
            logger.debug(f"üîì –û—Å–≤–æ–±–æ–∂–¥–µ–Ω –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ {device}")
    
    def _cleanup_device_memory(self, device: str):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞."""
        try:
            import torch
            
            if device.startswith("cuda"):
                with torch.cuda.device(device):
                    torch.cuda.empty_cache()
            elif device == "mps":
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
            
            gc.collect()
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞ {device}: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø—É–ª–∞ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤."""
        return {
            "total_devices": len(self.gpu_devices),
            "available_devices": self.available_devices.qsize(),
            "busy_devices": len(self.gpu_devices) - self.available_devices.qsize(),
            "model_name": self.model_name,
            "initialized": self._initialized
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø—É–ª –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤ (—Å–∏–Ω–≥–ª—Ç–æ–Ω)
_translator_pool: Optional[TranslatorPool] = None

def get_translator_pool() -> TranslatorPool:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø—É–ª–∞ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤."""
    global _translator_pool
    if _translator_pool is None:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ GPU —á—Ç–æ –∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        gpu_devices = config.diffusion.gpu_devices
        if not gpu_devices:
            gpu_devices = ["cpu"]  # Fallback
        _translator_pool = TranslatorPool(gpu_devices)
    return _translator_pool

class GPUPool:
    """–ü—É–ª GPU –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."""
    
    def __init__(self, gpu_devices: List[str]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É–ª–∞ GPU.
        
        Args:
            gpu_devices: –°–ø–∏—Å–æ–∫ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤
        """
        self.gpu_devices = gpu_devices
        self.pipelines: Dict[str, Any] = {}
        self.available_gpus = asyncio.Queue(maxsize=len(gpu_devices))
        self.generation_queue = asyncio.Queue(maxsize=config.diffusion.max_queue_size)
        self._initialized = False
        
        logger.info(f"üéÆ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω GPU –ø—É–ª —Å {len(gpu_devices)} —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏: {gpu_devices}")
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö pipeline –¥–ª—è GPU."""
        if self._initialized:
            return
        
        logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—Å–µ GPU...")
        
        for device in self.gpu_devices:
            try:
                pipeline = await self._load_pipeline_for_device(device)
                if pipeline:
                    self.pipelines[device] = pipeline
                    await self.available_gpus.put(device)
                    logger.info(f"‚úÖ Pipeline –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è {device}")
                else:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å pipeline –¥–ª—è {device}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ pipeline –¥–ª—è {device}: {e}")
        
        self._initialized = True
        logger.info(f"üöÄ GPU –ø—É–ª –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å {len(self.pipelines)} –∞–∫—Ç–∏–≤–Ω—ã–º–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏")
    
    async def _load_pipeline_for_device(self, device: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ pipeline –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞."""
        try:
            import torch
            from diffusers import (
                StableDiffusionXLPipeline, 
                StableDiffusionPipeline,
                DiffusionPipeline
            )
            
            model_name = config.diffusion.model
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø pipeline –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏
            if "flux" in model_name.lower():
                try:
                    from diffusers import FluxPipeline
                    pipeline_class = FluxPipeline
                    
                    load_kwargs = {
                        "torch_dtype": torch.bfloat16 if device != "cpu" else torch.float32,
                    }
                    
                    pipeline = pipeline_class.from_pretrained(model_name, **load_kwargs)
                    
                except ImportError:
                    logger.error(f"‚ùå FluxPipeline –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è {device}")
                    return None
                    
            elif "xl" in model_name.lower():
                pipeline_class = StableDiffusionXLPipeline
                
                load_kwargs = {
                    "torch_dtype": torch.float16 if device != "cpu" else torch.float32,
                    "safety_checker": None,
                    "requires_safety_checker": False
                }
                
                if device != "cpu":
                    load_kwargs["variant"] = "fp16"
                
                pipeline = pipeline_class.from_pretrained(model_name, **load_kwargs)
                
            elif "stable-diffusion" in model_name.lower():
                pipeline_class = StableDiffusionPipeline
                
                load_kwargs = {
                    "torch_dtype": torch.float16 if device != "cpu" else torch.float32,
                    "safety_checker": None,
                    "requires_safety_checker": False
                }
                
                if device != "cpu":
                    load_kwargs["variant"] = "fp16"
                
                pipeline = pipeline_class.from_pretrained(model_name, **load_kwargs)
                
            else:
                pipeline_class = DiffusionPipeline
                
                load_kwargs = {
                    "torch_dtype": torch.float16 if device != "cpu" else torch.float32,
                }
                
                pipeline = pipeline_class.from_pretrained(model_name, **load_kwargs)
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            pipeline = pipeline.to(device)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            self._apply_optimizations(pipeline, device)
            
            return pipeline
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ pipeline –¥–ª—è {device}: {e}")
            return None
    
    def _apply_optimizations(self, pipeline, device: str):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞."""
        try:
            if device.startswith("cuda"):
                if hasattr(pipeline, 'enable_memory_efficient_attention'):
                    pipeline.enable_memory_efficient_attention()
                    
                if config.diffusion.enable_xformers:
                    try:
                        pipeline.enable_xformers_memory_efficient_attention()
                    except Exception:
                        pass
                        
                if config.diffusion.enable_cpu_offload:
                    pipeline.enable_model_cpu_offload()
                    
            elif device == "mps":
                if hasattr(pipeline, 'enable_attention_slicing'):
                    pipeline.enable_attention_slicing()
                    
            else:  # CPU
                if hasattr(pipeline, 'enable_attention_slicing'):
                    pipeline.enable_attention_slicing()
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –¥–ª—è {device}: {e}")
    
    @asynccontextmanager
    async def acquire_gpu(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è GPU –∏–∑ –ø—É–ª–∞."""
        if not self._initialized:
            await self.initialize()
        
        # –ñ–¥–µ–º —Å–≤–æ–±–æ–¥–Ω—É—é GPU
        device = await self.available_gpus.get()
        pipeline = self.pipelines.get(device)
        
        if not pipeline:
            await self.available_gpus.put(device)
            raise RuntimeError(f"Pipeline –¥–ª—è {device} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        try:
            logger.debug(f"üîí –ü–æ–ª—É—á–µ–Ω –¥–æ—Å—Ç—É–ø –∫ {device}")
            yield device, pipeline
        finally:
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º GPU –≤ –ø—É–ª
            self._cleanup_device_memory(device)
            await self.available_gpus.put(device)
            logger.debug(f"üîì –û—Å–≤–æ–±–æ–∂–¥–µ–Ω {device}")
    
    def _cleanup_device_memory(self, device: str):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞."""
        try:
            import torch
            
            if device.startswith("cuda"):
                with torch.cuda.device(device):
                    torch.cuda.empty_cache()
            elif device == "mps":
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
            
            gc.collect()
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏ {device}: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø—É–ª–∞ GPU."""
        return {
            "total_gpus": len(self.gpu_devices),
            "available_gpus": self.available_gpus.qsize(),
            "busy_gpus": len(self.gpu_devices) - self.available_gpus.qsize(),
            "queue_size": self.generation_queue.qsize(),
            "max_queue_size": config.diffusion.max_queue_size,
            "initialized": self._initialized
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø—É–ª GPU (—Å–∏–Ω–≥–ª—Ç–æ–Ω)
_gpu_pool: Optional[GPUPool] = None

def get_gpu_pool() -> GPUPool:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø—É–ª–∞ GPU."""
    global _gpu_pool
    if _gpu_pool is None:
        gpu_devices = config.diffusion.gpu_devices
        if not gpu_devices:
            gpu_devices = ["cpu"]  # Fallback
        _gpu_pool = GPUPool(gpu_devices)
    return _gpu_pool

class ImageGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–æ–∑–¥—Ä–∞–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ AI –º–æ–¥–µ–ª—è–º–∏."""
    
    def __init__(self, progress_callback=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞.
        
        Args:
            progress_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
        """
        self.progress_callback = progress_callback
        self.gpu_pool = get_gpu_pool()
        self.translator_pool = get_translator_pool()
        
        logger.info("üé® –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω multi-GPU –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        logger.info(f"   –ú–æ–¥–µ–ª—å: {config.diffusion.model}")
        logger.info(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {config.diffusion.num_images}")
        logger.info(f"   GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: {config.diffusion.gpu_devices}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        if not self._check_dependencies():
            raise ImportError("–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

    def _get_expected_translation_time(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö."""
        return 2

    def _get_expected_generation_time(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Å–µ–∫—É–Ω–¥–∞—Ö."""
        model_name = config.diffusion.model.lower()
        num_images = config.diffusion.num_images
        steps = config.diffusion.num_inference_steps
        
        # –ë–∞–∑–æ–≤–æ–µ –≤—Ä–µ–º—è –Ω–∞ –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–¥–ª—è –æ–¥–Ω–æ–π GPU)
        if "flux" in model_name:
            base_time = 12  # FLUX –±—ã—Å—Ç—Ä–µ–µ
        elif "xl" in model_name:
            base_time = 8   # SDXL
        else:
            base_time = 5   # SD
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
        time_per_image = base_time * (steps / 20)
        total_time = time_per_image * num_images
        
        return int(total_time)

    async def _send_progress_message(self, message_key: str, **kwargs):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ —á–µ—Ä–µ–∑ callback."""
        if self.progress_callback:
            try:
                await self.progress_callback(message_key, **kwargs)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ: {e}")

    def _check_dependencies(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."""
        try:
            import torch
            import diffusers
            logger.info("‚úÖ –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–Ω—ã")
            return True
        except ImportError as e:
            logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {e}")
            logger.error("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch diffusers transformers")
            return False

    async def generate_birthday_image(self, text: str, user_id: int) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∑–¥—Ä–∞–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç–∏–Ω–æ–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU –ø—É–ª–∞.
        
        Args:
            text: –¢–µ–∫—Å—Ç –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        start_time = time.time()
        
        try:
            logger.info(f"üé® –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é {config.diffusion.num_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            output_dir = config.create_temp_images_dir(user_id)
            logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É–ª –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if not self.gpu_pool._initialized and config.diffusion.preload_model:
                await self.gpu_pool.initialize()
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU –ø—É–ª–∞
            images, content = await self._generate_with_gpu_pool(text)
            
            if images and len(images) > 0:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                saved_paths = []
                for i, image in enumerate(images):
                    if image:
                        filename = f"birthday_card_{i+1}.png"
                        image_path = Path(output_dir) / filename
                        
                        try:
                            image.save(image_path, "PNG", quality=95)
                            saved_paths.append(str(image_path))
                            logger.debug(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1}: {filename}")
                        except Exception as e:
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i+1}: {e}")
                
                if saved_paths:
                    generation_time = time.time() - start_time
                    logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(saved_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ {generation_time:.2f}—Å")
                    return output_dir, content
                else:
                    logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                    self._cleanup_directory(output_dir)
                    return None
            else:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                self._cleanup_directory(output_dir)
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
            if 'output_dir' in locals():
                self._cleanup_directory(output_dir)
            return None

    async def _generate_with_gpu_pool(self, text: str) -> Optional[List[Image.Image]]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU –ø—É–ª–∞.
        
        Args:
            text: –¢–µ–∫—Å—Ç –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è
            
        Returns:
            –°–ø–∏—Å–æ–∫ PIL Image –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç
            await self._send_progress_message(
                "translation_start",
                expected_time=self._get_expected_translation_time()
            )
            translation_start_time = time.time()

            prompt, content = await self._create_birthday_prompt(text)
            logger.info(f"üìù –ü—Ä–æ–º–ø—Ç: {prompt}.")
            
            translation_time = time.time() - translation_start_time
            await self._send_progress_message(
                "translation_done",
                actual_time=translation_time
            )

            await self._send_progress_message(
                "image_generation_start",
                num_images=config.diffusion.num_images,
                expected_time=self._get_expected_generation_time()
            )
            generation_start_time = time.time()

            # –ü–æ–ª—É—á–∞–µ–º GPU –∏–∑ –ø—É–ª–∞ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º
            async with self.gpu_pool.acquire_gpu() as (device, pipeline):
                logger.info(f"üéÆ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ {device}")
                
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                generation_params = self._get_generation_params(prompt)
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(
                    None,
                    self._run_pipeline,
                    pipeline,
                    generation_params
                )

                generation_time = time.time() - generation_start_time
                await self._send_progress_message(
                    "image_generation_done",
                    actual_time=generation_time
                )

                # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                if result and hasattr(result, 'images') and result.images:
                    images = result.images
                    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ {device}")
                    return images, content
                else:
                    logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
                    return None
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å GPU –ø—É–ª–æ–º: {e}")
            return None

    def _get_generation_params(self, prompt: str) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""
        params = {
            "prompt": prompt,
            "height": config.diffusion.height,
            "width": config.diffusion.width,
            "num_inference_steps": config.diffusion.num_inference_steps,
            "guidance_scale": config.diffusion.guidance_scale,
            "num_images_per_prompt": config.diffusion.num_images,
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –Ω–µ-FLUX –º–æ–¥–µ–ª–µ–π
        if "flux" not in config.diffusion.model.lower():
            if config.diffusion.negative_prompt:
                params["negative_prompt"] = config.diffusion.negative_prompt
        
        # –î–æ–±–∞–≤–ª—è–µ–º generator –¥–ª—è seed
        if config.diffusion.seed >= 0:
            import torch
            params["generator"] = torch.Generator().manual_seed(config.diffusion.seed)
        
        return params

    def _run_pipeline(self, pipeline, params: dict):
        """–ó–∞–ø—É—Å–∫ pipeline –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ."""
        try:
            import torch
            
            with torch.no_grad():
                return pipeline(**params)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è pipeline: {e}")
            return None

    async def _create_birthday_prompt(self, text: str) -> str:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        
        Args:
            text: –¢–µ–∫—Å—Ç –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è
            
        Returns:
            –ü—Ä–æ–º–ø—Ç –¥–ª—è AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        prompts_config = config.diffusion.prompts
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞
        if self.has_russian(text):
            content = await self.translate_text(text)
        else:
            content = text
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –ø–æ —à–∞–±–ª–æ–Ω—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if "style".lower() in content.lower():
            prompt = prompts_config.template_no_style.format(
                picture=prompts_config.base_picture,
                subject=prompts_config.subject,
                content=content,
            )
        else:        
            prompt = prompts_config.template_with_style.format(
                picture=prompts_config.base_picture,
                style=prompts_config.style,
                subject=prompts_config.subject,
                content=content,
            )
        
        return prompt, content

    def has_russian(self, text):
        return bool(re.search(r"[–∞-—è–ê-–Ø—ë–Å]", text))

    async def translate_text(self, text: str) -> str:
        """
        –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—É–ª–∞ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
            
        Returns:
            str: –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –∏–∑ –ø—É–ª–∞
            async with self.translator_pool.acquire_translator() as (device, tokenizer, model):
                logger.debug(f"üî§ –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ {device}")
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(
                    None,
                    self._translate_sync,
                    tokenizer,
                    model,
                    text
                )
                
                return result if result else text
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞: {e}")
            return text  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ
    
    def _translate_sync(self, tokenizer, model, text: str) -> str:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ (–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ).
        
        Args:
            tokenizer: –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä –º–æ–¥–µ–ª–∏
            model: –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
            
        Returns:
            str: –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            tokens = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ç–æ–∫–µ–Ω—ã –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —á—Ç–æ –∏ –º–æ–¥–µ–ª—å
            if hasattr(model, 'device'):
                tokens = {k: v.to(model.device) for k, v in tokens.items()}
            
            translated = model.generate(**tokens)
            result = tokenizer.decode(translated[0], skip_special_tokens=True)
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return text

    def _cleanup_directory(self, directory_path: str) -> None:
        """
        –û—á–∏—Å—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –µ—ë —É–¥–∞–ª–µ–Ω–∏–µ.
        
        Args:
            directory_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        """
        try:
            dir_path = Path(directory_path)
            if dir_path.exists() and dir_path.is_dir():
                shutil.rmtree(dir_path)
                logger.debug(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {directory_path}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {directory_path}: {e}")

    def cleanup_temp_files(self, max_age_hours: int = 24) -> None:
        """
        –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π.
        
        Args:
            max_age_hours: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç —Ñ–∞–π–ª–æ–≤ –≤ —á–∞—Å–∞—Ö
        """
        try:
            temp_dir = Path(config.paths.temp_images)
            if not temp_dir.exists():
                return
            
            max_age_seconds = max_age_hours * 3600
            current_time = time.time()
            
            cleaned_count = 0
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
            for dir_path in temp_dir.glob("birthday_cards_*"):
                if dir_path.is_dir():
                    if current_time - dir_path.stat().st_mtime > max_age_seconds:
                        try:
                            shutil.rmtree(dir_path)
                            cleaned_count += 1
                            logger.debug(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {dir_path.name}")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {dir_path}: {e}")
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            for file_path in temp_dir.glob("birthday_card_*.png"):
                if current_time - file_path.stat().st_mtime > max_age_seconds:
                    try:
                        file_path.unlink()
                        cleaned_count += 1
                        logger.debug(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª: {file_path.name}")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª {file_path}: {e}")
            
            if cleaned_count > 0:
                logger.info(f"üßπ –û—á–∏—â–µ–Ω–æ {cleaned_count} —Å—Ç–∞—Ä—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")

    def get_image_paths_from_dir(self, directory_path: str) -> List[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–µ–π –∫–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
        
        Args:
            directory_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        """
        try:
            dir_path = Path(directory_path)
            if not dir_path.exists() or not dir_path.is_dir():
                logger.warning(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {directory_path}")
                return []
            
            # –ò—â–µ–º PNG —Ñ–∞–π–ª—ã —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º birthday_card_*
            image_paths = []
            for i in range(1, config.diffusion.num_images + 1):
                image_path = dir_path / f"birthday_card_{i}.png"
                if image_path.exists():
                    image_paths.append(str(image_path))
                else:
                    logger.warning(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {image_path}")
            
            logger.debug(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {directory_path}")
            return image_paths
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º: {e}")
            return []

    def get_status(self) -> dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        
        Returns:
            dict: –°—Ç–∞—Ç—É—Å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        """
        gpu_status = self.gpu_pool.get_status()
        translator_status = self.translator_pool.get_status()
        
        return {
            "local_diffusion_available": self._check_dependencies(),
            "local_model": config.diffusion.model,
            "gpu_pool_status": gpu_status,
            "translator_pool_status": translator_status,
            "num_images_per_generation": config.diffusion.num_images,
            "dependencies_installed": self._check_dependencies()
        }
    