"""
–ú–æ–¥—É–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª–æ–∫–∞–ª—å–Ω—ã—Ö Stable Diffusion –º–æ–¥–µ–ª–µ–π.
–°–æ–∑–¥–∞–µ—Ç –ø–æ–∑–¥—Ä–∞–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ —Å AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π.
"""

import asyncio
import time
import gc
import re
import shutil
from pathlib import Path
from typing import Optional, List
from PIL import Image, ImageDraw, ImageFont

from src.utils.config import config
from src.utils.logger import get_image_logger
from transformers import MarianMTModel, MarianTokenizer

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
logger = get_image_logger()

class ImageGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–æ–∑–¥—Ä–∞–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ AI –º–æ–¥–µ–ª—è–º–∏."""
    
    def __init__(self, progress_callback=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞.
        
        Args:
            progress_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
        """
        self.device = None
        self.pipeline = None
        self.model_loaded = False
        self.progress_callback = progress_callback
        
        logger.info("üé® –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        logger.info(f"   –ú–æ–¥–µ–ª—å: {config.diffusion.model}")
        logger.info(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {config.diffusion.num_images}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.device = self._get_device()
        logger.info(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        if not self._check_dependencies():
            raise ImportError("–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

    def _get_device(self):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π."""
        try:
            import torch
        except ImportError:
            logger.error("‚ùå PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            return "cpu"
            
        if config.diffusion.device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
                try:
                    device_name = torch.cuda.get_device_name(0)
                    logger.info(f"üöÄ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ CUDA GPU: {device_name}")
                except:
                    logger.info("üöÄ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞")
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
                logger.info("üçé –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Apple Silicon GPU (MPS)")
            else:
                device = "cpu"
                logger.info("üíª –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU")
        else:
            device = config.diffusion.device
            
        return device

    def _get_expected_model_load_time(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö."""
        model_name = config.diffusion.model.lower()
        
        if "flux" in model_name:
            if self.device == "cuda":
                return 15  # FLUX –Ω–∞ GPU
            else:
                return 40  # FLUX –Ω–∞ CPU
        elif "xl" in model_name:
            if self.device == "cuda":
                return 30  # SDXL –Ω–∞ GPU
            else:
                return 90   # SDXL –Ω–∞ CPU
        else:
            if self.device == "cuda":
                return 20   # SD –Ω–∞ GPU
            else:
                return 60   # SD –Ω–∞ CPU

    def _get_expected_translation_time(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö."""
        return 2

    def _get_expected_generation_time(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Å–µ–∫—É–Ω–¥–∞—Ö."""
        model_name = config.diffusion.model.lower()
        num_images = config.diffusion.num_images
        steps = config.diffusion.num_inference_steps
        
        # –ë–∞–∑–æ–≤–æ–µ –≤—Ä–µ–º—è –Ω–∞ –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if "flux" in model_name:
            if self.device == "cuda":
                base_time = 12  # FLUX –Ω–∞ GPU - –±—ã—Å—Ç—Ä–µ–µ
            else:
                base_time = 50  # FLUX –Ω–∞ CPU
        elif "xl" in model_name:
            if self.device == "cuda":
                base_time = 8   # SDXL –Ω–∞ GPU
            else:
                base_time = 45  # SDXL –Ω–∞ CPU
        else:
            if self.device == "cuda":
                base_time = 5   # SD –Ω–∞ GPU
            else:
                base_time = 30  # SD –Ω–∞ CPU
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        time_per_image = base_time * (steps / 20)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 20 —à–∞–≥–∞–º
        total_time = time_per_image * num_images
        
        return int(total_time)

    async def _send_progress_message(self, message_key: str, **kwargs):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ —á–µ—Ä–µ–∑ callback."""
        if self.progress_callback:
            try:
                await self.progress_callback(message_key, **kwargs)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ: {e}")

    def _check_dependencies(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."""
        try:
            import torch
            import diffusers
            logger.info("‚úÖ –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–Ω—ã")
            return True
        except ImportError as e:
            logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {e}")
            logger.error("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch diffusers transformers")
            return False

    async def generate_birthday_image(self, text: str, user_id: int) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∑–¥—Ä–∞–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç–∏–Ω–æ–∫.
        
        Args:
            text: –¢–µ–∫—Å—Ç –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        start_time = time.time()
        
        try:
            logger.info(f"üé® –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é {config.diffusion.num_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            output_dir = config.create_temp_images_dir(user_id)
            logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
            
            images = await self._generate_with_diffusion(text)
            
            if images and len(images) > 0:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                saved_paths = []
                for i, image in enumerate(images):
                    if image:
                        filename = f"birthday_card_{i+1}.png"
                        image_path = Path(output_dir) / filename
                        
                        try:
                            image.save(image_path, "PNG", quality=95)
                            saved_paths.append(str(image_path))
                            logger.debug(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1}: {filename}")
                        except Exception as e:
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i+1}: {e}")
                
                if saved_paths:
                    generation_time = time.time() - start_time
                    logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(saved_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ {generation_time:.2f}—Å")
                    
                    # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
                    self._cleanup_memory()
                    
                    return output_dir
                else:
                    logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                    self._cleanup_directory(output_dir)
                    return None
            else:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                self._cleanup_directory(output_dir)
                return None
                
        except Exception as e:
            logger.error(
                f"error={e},"
                f"context={{"
                f"method: 'generate_birthday_image',"
                f"user_id: {user_id},"
                f"text_length: {len(text)}"
                f"}}"
            )
            # –û—á–∏—â–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ
            if 'output_dir' in locals():
                self._cleanup_directory(output_dir)
            return None

    def _cleanup_directory(self, directory_path: str) -> None:
        """
        –û—á–∏—Å—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –µ—ë —É–¥–∞–ª–µ–Ω–∏–µ.
        
        Args:
            directory_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        """
        try:
            dir_path = Path(directory_path)
            if dir_path.exists() and dir_path.is_dir():
                shutil.rmtree(dir_path)
                logger.debug(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {directory_path}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {directory_path}: {e}")

    async def _load_diffusion_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π diffusion –º–æ–¥–µ–ª–∏."""
        if self.model_loaded:
            return True
            
        try:
            import torch
            from diffusers import (
                StableDiffusionXLPipeline, 
                StableDiffusionPipeline,
                DiffusionPipeline
            )
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –Ω–∞—á–∞–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
            await self._send_progress_message(
                "model_loading_start",
                expected_time=self._get_expected_model_load_time()
            )
            
            logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π diffusion –º–æ–¥–µ–ª–∏...")
            start_time = time.time()
            
            model_name = config.diffusion.model
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø pipeline –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏
            if "flux" in model_name.lower():
                logger.info("üîß –ó–∞–≥—Ä—É–∑–∫–∞ FLUX pipeline...")
                try:
                    from diffusers import FluxPipeline
                    pipeline_class = FluxPipeline
                    
                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è FLUX
                    load_kwargs = {
                        "torch_dtype": torch.bfloat16 if self.device != "cpu" else torch.float32,
                    }
                    
                    # FLUX –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç device_map="auto", –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
                    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ FLUX –º–æ–¥–µ–ª–∏ {model_name}...")
                    self.pipeline = pipeline_class.from_pretrained(model_name, **load_kwargs)
                    
                except ImportError as e:
                    logger.error(f"error: {e}")
                    logger.error(f"‚ùå FluxPipeline –Ω–µ –Ω–∞–π–¥–µ–Ω. –û–±–Ω–æ–≤–∏—Ç–µ diffusers: pip install diffusers>=0.30.0")
                    import diffusers
                    logger.info(f"diffusers: {diffusers.__version__}")
                    return False
                    
            elif "xl" in model_name.lower():
                logger.info("üîß –ó–∞–≥—Ä—É–∑–∫–∞ SDXL pipeline...")
                pipeline_class = StableDiffusionXLPipeline
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–ª—è SDXL
                load_kwargs = {
                    "torch_dtype": torch.float16 if self.device != "cpu" else torch.float32,
                    "safety_checker": None,
                    "requires_safety_checker": False
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º variant –¥–ª—è fp16 –º–æ–¥–µ–ª–µ–π
                if self.device != "cpu":
                    load_kwargs["variant"] = "fp16"
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                self.pipeline = pipeline_class.from_pretrained(model_name, **load_kwargs)
                
            elif "stable-diffusion" in model_name.lower():
                logger.info("üîß –ó–∞–≥—Ä—É–∑–∫–∞ SD pipeline...")
                pipeline_class = StableDiffusionPipeline
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–ª—è SD
                load_kwargs = {
                    "torch_dtype": torch.float16 if self.device != "cpu" else torch.float32,
                    "safety_checker": None,
                    "requires_safety_checker": False
                }
                
                if self.device != "cpu":
                    load_kwargs["variant"] = "fp16"
                
                self.pipeline = pipeline_class.from_pretrained(model_name, **load_kwargs)
                
            else:
                logger.info("üîß –ó–∞–≥—Ä—É–∑–∫–∞ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ pipeline...")
                pipeline_class = DiffusionPipeline
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ pipeline
                load_kwargs = {
                    "torch_dtype": torch.float16 if self.device != "cpu" else torch.float32,
                }
                
                self.pipeline = pipeline_class.from_pretrained(model_name, **load_kwargs)
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.pipeline = self.pipeline.to(self.device)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            self._apply_optimizations()
            
            self.model_loaded = True
            load_time = time.time() - start_time
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏
            logger.info(f"model_loading_done")
            await self._send_progress_message(
                "model_loading_done",
                actual_time=load_time
            )
            
            logger.info(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.1f}—Å")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return False

    def _apply_optimizations(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤."""
        try:
            if self.device == "cuda":
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è CUDA
                if hasattr(self.pipeline, 'enable_memory_efficient_attention'):
                    self.pipeline.enable_memory_efficient_attention()
                    
                if config.diffusion.enable_xformers:
                    try:
                        self.pipeline.enable_xformers_memory_efficient_attention()
                        logger.info("‚úÖ –í–∫–ª—é—á–µ–Ω–∞ xformers –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")
                    except Exception:
                        logger.warning("‚ö†Ô∏è xformers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                        
                if config.diffusion.enable_cpu_offload:
                    self.pipeline.enable_model_cpu_offload()
                    logger.info("‚úÖ –í–∫–ª—é—á–µ–Ω–∞ CPU —Ä–∞–∑–≥—Ä—É–∑–∫–∞")
                    
            elif self.device == "mps":
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Apple Silicon
                if hasattr(self.pipeline, 'enable_attention_slicing'):
                    self.pipeline.enable_attention_slicing()
                    logger.info("‚úÖ –í–∫–ª—é—á–µ–Ω–∞ attention slicing –¥–ª—è MPS")
                    
            else:  # CPU
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è CPU
                if hasattr(self.pipeline, 'enable_attention_slicing'):
                    self.pipeline.enable_attention_slicing()
                    logger.info("‚úÖ –í–∫–ª—é—á–µ–Ω–∞ attention slicing –¥–ª—è CPU")
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π: {e}")

    def _cleanup_memory(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""
        try:
            import torch
            
            if self.device == "cuda":
                torch.cuda.empty_cache()
            elif self.device == "mps":
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
            
            gc.collect()
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏: {e}")

    async def _generate_with_diffusion(self, text: str) -> Optional[List[Image.Image]]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é diffusion.
        
        Args:
            text: –¢–µ–∫—Å—Ç –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è
            
        Returns:
            –°–ø–∏—Å–æ–∫ PIL Image –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not self.model_loaded:
            if not await self._load_diffusion_model():
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å")
                return None
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é

            import torch
            
            logger.info(f"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {config.diffusion.num_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é...")
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

            await self._send_progress_message(
                "translation_start",
                expected_time=self._get_expected_translation_time()
            )
            translation_start_time = time.time()

            prompt = self._create_birthday_prompt(text)
            logger.info(f"üìù –ü—Ä–æ–º–ø—Ç: {prompt}...")
            
            translation_time = time.time() - translation_start_time
            await self._send_progress_message(
                "translation_done",
                actual_time=translation_time
            )

            await self._send_progress_message(
                "image_generation_start",
                num_images=config.diffusion.num_images,
                expected_time=self._get_expected_generation_time()
            )
            generation_start_time = time.time()

            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Ä–∞–∑–Ω—ã–µ –¥–ª—è FLUX –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π)
            if "flux" in config.diffusion.model.lower():
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è FLUX
                generation_params = {
                    "prompt": prompt,
                    "height": config.diffusion.height,
                    "width": config.diffusion.width,
                    "num_inference_steps": config.diffusion.num_inference_steps,
                    "guidance_scale": config.diffusion.guidance_scale,
                    "num_images_per_prompt": config.diffusion.num_images,
                }
                
                # FLUX –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å negative_prompt
                logger.debug("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å FLUX –º–æ–¥–µ–ª—å—é...")
                
            else:
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Stable Diffusion
                generation_params = {
                    "prompt": prompt,
                    "height": config.diffusion.height,
                    "width": config.diffusion.width,
                    "num_inference_steps": config.diffusion.num_inference_steps,
                    "guidance_scale": config.diffusion.guidance_scale,
                    "num_images_per_prompt": config.diffusion.num_images,
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è SD
                if config.diffusion.negative_prompt:
                    generation_params["negative_prompt"] = config.diffusion.negative_prompt
            
            # –î–æ–±–∞–≤–ª—è–µ–º generator –¥–ª—è seed
            if config.diffusion.seed >= 0:
                generation_params["generator"] = torch.Generator().manual_seed(config.diffusion.seed)
            
            logger.debug(f"üéõÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: steps={generation_params['num_inference_steps']}, "
                        f"guidance={generation_params['guidance_scale']}, "
                        f"images={generation_params['num_images_per_prompt']}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                self._run_pipeline,
                generation_params
            )

            generation_time = time.time() - generation_start_time
            await self._send_progress_message(
                "image_generation_done",
                actual_time=generation_time
            )

            # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if result and hasattr(result, 'images') and result.images:
                images = result.images
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ª–æ–∫–∞–ª—å–Ω–æ")
                return images
            else:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å diffusion: {e}")
            return None

    def _run_pipeline(self, params: dict):
        """–ó–∞–ø—É—Å–∫ pipeline –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ."""
        try:
            import torch
            
            with torch.no_grad():  # –≠–∫–æ–Ω–æ–º–∏–º –ø–∞–º—è—Ç—å
                return self.pipeline(**params)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è pipeline: {e}")
            return None

    def _create_birthday_prompt(self, text: str) -> str:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        
        Args:
            text: –¢–µ–∫—Å—Ç –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è
            
        Returns:
            –ü—Ä–æ–º–ø—Ç –¥–ª—è AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        prompts_config = config.diffusion.prompts
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞
        if self.has_russian(text):
            content = self.translate_text(text)
        else:
            content = text
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –ø–æ —à–∞–±–ª–æ–Ω—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if "style".lower() in content.lower():
            prompt = prompts_config.template_no_style.format(
                picture=prompts_config.base_picture,
                subject=prompts_config.subject,
                content=content,
            )
        else:        
            prompt = prompts_config.template_with_style.format(
                picture=prompts_config.base_picture,
                style=prompts_config.style,
                subject=prompts_config.subject,
                content=content,
            )
        
        return prompt

    def has_russian(self, text):
        return bool(re.search(r"[–∞-—è–ê-–Ø—ë–Å]", text))

    def translate_text(self, text: str) -> str:
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
        model_name = "Helsinki-NLP/opus-mt-ru-en"
        tokenizer = MarianTokenizer.from_pretrained(model_name)
        model = MarianMTModel.from_pretrained(model_name)

        tokens = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        translated = model.generate(**tokens)
        result = tokenizer.decode(translated[0], skip_special_tokens=True)

        return result

    def cleanup_temp_files(self, max_age_hours: int = 24) -> None:
        """
        –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π.
        
        Args:
            max_age_hours: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç —Ñ–∞–π–ª–æ–≤ –≤ —á–∞—Å–∞—Ö
        """
        try:
            temp_dir = Path(config.paths.temp_images)
            if not temp_dir.exists():
                return
            
            max_age_seconds = max_age_hours * 3600
            current_time = time.time()
            
            cleaned_count = 0
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
            for dir_path in temp_dir.glob("birthday_cards_*"):
                if dir_path.is_dir():
                    if current_time - dir_path.stat().st_mtime > max_age_seconds:
                        try:
                            shutil.rmtree(dir_path)
                            cleaned_count += 1
                            logger.debug(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {dir_path.name}")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {dir_path}: {e}")
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            for file_path in temp_dir.glob("birthday_card_*.png"):
                if current_time - file_path.stat().st_mtime > max_age_seconds:
                    try:
                        file_path.unlink()
                        cleaned_count += 1
                        logger.debug(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª: {file_path.name}")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª {file_path}: {e}")
            
            if cleaned_count > 0:
                logger.info(f"üßπ –û—á–∏—â–µ–Ω–æ {cleaned_count} —Å—Ç–∞—Ä—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")

    def get_image_paths_from_dir(self, directory_path: str) -> List[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–µ–π –∫–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
        
        Args:
            directory_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        """
        try:
            dir_path = Path(directory_path)
            if not dir_path.exists() or not dir_path.is_dir():
                logger.warning(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {directory_path}")
                return []
            
            # –ò—â–µ–º PNG —Ñ–∞–π–ª—ã —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º birthday_card_*
            image_paths = []
            for i in range(1, config.diffusion.num_images + 1):
                image_path = dir_path / f"birthday_card_{i}.png"
                if image_path.exists():
                    image_paths.append(str(image_path))
                else:
                    logger.warning(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {image_path}")
            
            logger.debug(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {directory_path}")
            return image_paths
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º: {e}")
            return []

    def get_status(self) -> dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        
        Returns:
            dict: –°—Ç–∞—Ç—É—Å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        """
        return {
            "local_diffusion_available": self._check_dependencies(),
            "local_model": config.diffusion.model,
            "local_device": self.device,
            "local_model_loaded": self.model_loaded,
            "num_images_per_generation": config.diffusion.num_images,
            "dependencies_installed": self._check_dependencies()
        }
        